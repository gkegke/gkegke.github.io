# AI generating data for other AI - what you might be discounting.

> “The most powerful force in the universe is compound interest.” - Albert Einstein

Let's start by understanding two components of how deep learning AI models performance scale at a high level.

### Parameters

The number of parameters a model - at least one that aims to be very generally intelligent, one that can handle a diverse set of complex tasks - has definitely matters a lot (GPT-4 is estimated to be in the trillions while GPT-3 has 175 billion parameters).

Parameters capture the complex relationships hidden within data. Having many parameters mean you're less likely to get stuck in local optima (suboptimal solutions) since there are so many possible ways to move in high N-dimensional space, i.e. there's always a good route to travel to the global optima (optimal solution) or at least something close to it.

Imagine you are a 1 dimensional being walking across a road and encounter a big 'rock' that didn't use to be there in the past, leaving you stuck with no way to continue forward and get to your home. A 2 dimensional creature on the other hand could just jump over the big rock. A 3 dimensional creature might move to the side. A 4 dimensional creature might go back in time to a day where the rock wasn't there, and so on. If there are countless ways for you to move, there's rarely going to be anything significant enough to block you from getting to where you need to.

Hence it is fair to say if you want to handle more complex problems, the base level of parameters need to be sufficient otherwise it's easy to get stuck with sub-optimal solutions in the same way some problems are impossible to solve if you're missing a piece of info/perspective, and the more perspectives you have the easier it is to find one that allows you to get to a good/the answer. 

Like always, there are diminishing returns to things like this. So endlessly increasing parameters won't always result in better performance and can even result in declines/overfitting, in the same way, many problems can be solved by hand and you don't need to use a calculator, in fact, bringing the calculator can add extra complexity to the problem and even distract you or open you to flawed results and mistakes, but again at a high level you need a sufficiently high parameter count for a given level of task, and not meeting the requirements will result in poor performance.

# High quality data

Increasing the amount of (high quality) data, or the number of tokens in the case of a LLM model (GPT3 was trained on over 570GB of text data, which contains approximately 175 billion tokens) is also naturally beneficial, in the same way regardless of the task you need a base level of practise and experience to improve. The more diverse, highly accurate, and large scale the data the better the model will be at capturing the complexity hidden in data and the more resiliant it will be when facing new challenges, in the same way an experienced athlete with many well practised skills and techniques has a much higher chance of making and choosing the right play. 

Recent results in regards to LLM's have shown that one of the easiest ways to improve performance is increasing the amount of high quality data as a lot of the gains in performance in recent years could be said to be a result of simply throwing large amounts of high quality data at the problem.

# Human generated data is expensive to collect, why not just replace the human?

As new generation AI models pass certain thresholds of performance and accuracy, the option of using them to generate the data itself is becoming more and more viable.

Experiments such as Alpaca already show that you can get respectably peforming models this way, especially if you factor in the significantly reduced costs it took to train it.

LLM's, languaged focused AI are only one avenue, there are other powerful AI models focused on other problems from image recognition, to robotics.

These next generation AI's can borrow from each other, for example use a vision focused model's image recognition capabilities to assess the accuracy of code generated by a language generation focused model that produces complex user interfaces, synergistically experimenting and producing vast amounts of new high-quality data. 

As newer and newer models are released that specialize in different things, each generating high quality data they specialize in.. maybe similar to the human brain which itself is composed up of various regions that specialize in different things (from language, to emotion recognition, to spatial awareness, to planning), these models could boost each others development at an exponential rate.

AI now can produce high-quality data across many dimensions for itself, this generation of high-quality data is no different from the 'movie like' criteria of the singularity, whereby the AI can improve itself by itself - just without the writing of code aspect that era of AI thinking was based on, where people assumed AI would be written top down like conditional logic, and not caused to emerge from a much more bottom up deep learning approach.
