# 4 dimensions of compounding growth in AI

A lot of naysayers of AI are actually right.

Modern AI models/systems **are** actually pretty simplistic and elementary compared to the complexity of the human brain.

The human brain **is** still far ahead of current generation AI models in terms of processing ability for example. It's also multi-modal, has many efficient specialised segments/networks, and doesn't think of the world in high level things like 'tokens' that limit their foundational capabilities etc etc. 

Instead of being pessimistic as a consequence of all this however, the fact models are performing so well already given the simplistic-ness of current approaches is actually a great reason to be optimistic.

Especially as given their simpleness, current models can still improve in many dimensions, and the more dimensions there are for improvement the better due to the fact that growth in one dimension, in the long run tends to boost growth in others multiplictively.

Here are 4 well defined clear dimensions that are growing currently (and why I am an AGI sooner than you'd think optimist).

### Dimension 1: Pretraining (Education)

Pretraining in AI is akin to human education—building foundational knowledge through exposure to vast datasets. It lays the groundwork for a model’s capabilities, enabling them to generalize across a broad range of tasks.

#### **Why Pretraining Will Continue to Improve Soon:**
1. **Expansion of Multi-modal Training**:  
   Incorporating multi-modal datasets (text, images, audio, and video) allows models to learn from diverse data sources, similar to how humans integrate sensory input. This capability is already growing rapidly, with foundational models like GPT-4 incorporating multi-modal capabilities.
   
2. **Synthetic Data Refinement**:  
   New advancements in synthetic data generation, combined with improved real-world data sampling, are making synthetic data an increasingly valuable training resource.

3. **More Efficient Architectures**:  
   Innovations like sparse models (activating only necessary neurons) and dynamic tokenization allow models to extract more value from data without scaling linearly in complexity.

4. **Self-Supervised Learning (SSL)**:  
   SSL methods, where models learn patterns without explicit labels, are enabling more effective utilization of unlabeled data—a resource that remains abundant.

5. **Knowledge Integration Systems**:  
   Pretraining is evolving to incorporate structured, domain-specific knowledge (e.g., scientific databases or ontologies), enhancing domain-specific intelligence with minimal additional compute.

### Dimension 2: Specialized Application (Reinforcement Learning)

Reinforcement learning (RL) fine-tunes models for specialized tasks, much like on-the-job training for humans. It ensures AI systems not only possess theoretical knowledge but also adapt to dynamic environments and domain-specific challenges.


#### **Why RL Will See Tangible Growth Soon:**
1. **Sim-to-Real Advancements**:  
   Improvements in simulation environments enable RL models to learn complex tasks in virtual worlds and then transfer those skills to real-world applications with minimal tuning.

2. **Reinforcement Learning from Human Feedback (RLHF)**:  
   Already applied in fine-tuning models like GPT-4, RLHF aligns AI outputs with human preferences and ethical considerations, increasing their reliability.

3. **Hierarchical Learning**:  
   Breaking tasks into smaller, more manageable subtasks is making RL more scalable and practical, mirroring human approaches to complex problems.

4. **Continuous Learning**:  
   Emerging algorithms are enabling RL models to update their skills dynamically without forgetting previously learned tasks, addressing a common limitation in current approaches.

5. **Reward Engineering**:  
   Advancements in designing nuanced reward signals are enabling RL models to learn more effectively, even in scenarios where rewards are sparse or ambiguous.

### Dimension 3: Inference-Time Scaling (Reasoning and Problem Solving)

Pretrained and specialized models must perform well at inference time—the phase where they are deployed to solve problems. Recent developments in real-time reasoning and problem-solving suggest significant near-term growth.

#### **Why Inference-Time Scaling Will Improve Soon:**
1. **Better External Memory Integration**:  
   Techniques such as retrieval-augmented generation (RAG) and external knowledge bases allow models to dynamically access relevant information, effectively "expanding" their memory during reasoning.

2. **Iterative Reasoning**:  
   Models are being equipped with multi-step reasoning capabilities, enabling them to revisit and refine their outputs, much like human critical thinking.

3. **Context Expansion**:  
   Larger context windows (e.g., OpenAI's recent strides in extending context lengths) allow models to analyze and reason over broader datasets in a single session.

4. **Modular Reasoning Frameworks**:  
   Combining models specialized in different tasks during inference (e.g., a model for logic, another for language) enhances problem-solving versatility without requiring monolithic systems.

5. **Interactive Problem Solving**:  
   Real-time interactions between users and models are allowing AI to adapt responses based on iterative feedback, tailoring solutions dynamically.


### Dimension 4: Compute (Hardware and Software Advances)

Compute remains the backbone of AI development, enabling faster, larger, and more efficient training and inference. Improvements in hardware and software infrastructure are driving exponential growth in computational capabilities.

#### **Why Compute Will Continue Growing Soon:**
1. **Efficient AI Hardware**:  
   Next-generation GPUs, TPUs, and AI-specific chips (e.g., NVIDIA’s H100s, B200s) are optimizing compute for AI workloads, offering better performance per watt and lowering operational costs.

2. **Model Compression Techniques**:  
   Advances in quantization, pruning, and distillation are enabling the deployment of large models on smaller, cheaper hardware without significant performance degradation.

3. **Distributed Training Systems**:  
   New frameworks for distributed training (e.g., DeepSpeed, ZeRO) are making it easier to scale models across large clusters of machines.

4. **Algorithmic Efficiency**:  
   Innovations in training algorithms (e.g., sparse attention and low-rank factorization) are reducing the compute requirements for training and inference without sacrificing performance.

5. **Scalable Cloud Infrastructure**:  
   Hyperscalers like AWS, Google Cloud, and Azure are enhancing their AI-focused services, making large-scale compute accessible to a broader range of organizations.

------------

These are all dimensions of growth, where the probability of respectable progress are high.

Even in the worst case scenario, it's likely the performance of current generation models should improve drastically within the next few years.

Naysayers are right in some aspects, but they also tend to be unable to see all the possibilities for growth in other dimensions.

Yes, it's common for many sci-fi sounding technologies like nuclear fusion, whatever/whatever to end up havng many bottle-necks and progress appearing stuck, but the dimensions for growth in those technologies clearly are much more limited than in this scenario case by case analysis wise.
